#!/usr/bin/env ruby

require 'ollama'
include Ollama
require 'tins'
include Tins::GO
require 'json'

# Returns the contents of a file or string, or a default value if neither is provided.
#
# @param [String] path_or_content The path to a file or a string containing
#                 the content.
#
# @param [String] default The default value to return if no valid input is
#                 given. Defaults to nil.
#
# @return [String] The contents of the file, the string, or the default value.
#
# @example Get the contents of a file
#   get_file_argument('path/to/file')
#
# @example Use a string as content
#   get_file_argument('string content')
#
# @example Return a default value if no valid input is given
#   get_file_argument(nil, default: 'default content')
def get_file_argument(path_or_content, default: nil)
  if path_or_content.present? && path_or_content.size < 2 ** 15 &&
      File.basename(path_or_content).size < 2 ** 8 &&
      File.exist?(path_or_content)
    then
    File.read(path_or_content)
  elsif path_or_content.present?
    path_or_content
  else
    default
  end
end

def usage
  puts <<~EOT
    Usage: #{File.basename($0)} [OPTIONS]

      -u URL         the ollama base url, OLLAMA_URL
      -m MODEL       the ollama model to chat with, OLLAMA_MODEL
      -M OPTIONS     the ollama model options to use, OLLAMA_MODEL_OPTIONS
      -s SYSTEM      the system prompt to use as a file, OLLAMA_SYSTEM
      -p PROMPT      the user prompt to use as a file, OLLAMA_PROMPT
                     if it contains %{stdin} it is substituted by stdin input
      -P VARIABLE    sets prompt var %{foo} to "bar" if VARIABLE is foo=bar
      -H HANDLER     the handler to use for the response, defaults to Print
      -S             use streaming for generation
      -h             this help

  EOT
  exit 0
end

opts = go 'u:m:M:s:p:P:H:Sh', defaults: { ?H => 'Print', ?M => '{}' }

opts[?h] and usage

base_url = opts[?u] || ENV['OLLAMA_URL'] || 'http://%s' % ENV.fetch('OLLAMA_HOST')
model    = opts[?m] || ENV.fetch('OLLAMA_MODEL', 'llama3.1')
options  = Ollama::Options.from_hash(JSON(
  get_file_argument(opts[?M], default: ENV['OLLAMA_MODEL_OPTIONS'])
))
system   = get_file_argument(opts[?s], default: ENV['OLLAMA_SYSTEM'])
prompt   = get_file_argument(opts[?p], default: ENV['OLLAMA_PROMPT'])

if prompt.nil?
  prompt = STDIN.read
else
  vars = prompt.scan(/%\{([^}]+)\}/).inject([], &:concat).uniq.map(&:to_sym)
  stdin = (STDIN.read if vars.include?(:stdin)).to_s
  values = opts[?P].to_a.inject({ stdin: }) { |h, pair|
    n, v = pair.split(?=, 2)
    h.merge(n.to_sym => v)
  }
  prompt = prompt % values
end

if ENV['DEBUG'].to_i == 1
  puts <<~EOT
    base_url = #{base_url.inspect}
    model    = #{model.inspect}
    system   = #{system.inspect}
    prompt   = #{prompt.inspect}
    options  = #{options.to_json}
  EOT
end

Client.new(base_url:, read_timeout: 120).generate(
  model:,
  system:,
  prompt:,
  options:,
  stream: !!opts[?S],
  &Object.const_get(opts[?H])
)
